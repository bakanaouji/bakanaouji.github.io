<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Kenshi Abe">
    <meta name="description" content="https://bakanaouji.github.io/">
    <meta name="keywords" content="portfolio,researcher,personal">

    <meta property="og:site_name" content="Kenshi Abe">
    <meta property="og:title" content="
   - Kenshi Abe
">
    <meta property="og:description" content="">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://bakanaouji.github.io/works/">
    <meta property="og:image" content="https://bakanaouji.github.io/&lt;no value&gt;">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="https://bakanaouji.github.io/works/">
    <meta name="twitter:image" content="https://bakanaouji.github.io/&lt;no value&gt;">

    <base href="https://bakanaouji.github.io/works/">
    <title>
   - Kenshi Abe
</title>

    <link rel="canonical" href="https://bakanaouji.github.io/works/">
    
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
    
    <link  rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700">
    <link rel="stylesheet" href="/css/normalize.min.css">
    <link rel="stylesheet" href="/css/style.min.css">

    

    
      <link rel="stylesheet" href="https://bakanaouji.github.io/custom.css">
    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    
      <link rel="alternate" href="https://bakanaouji.github.io/index.xml" type="application/rss+xml" title="Kenshi Abe">
      <link href="https://bakanaouji.github.io/index.xml" rel="feed" type="application/rss+xml" title="Kenshi Abe" />
    

    <meta name="generator" content="Hugo 0.121.1">
  </head>

  <body class="">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">Kenshi Abe</a>
    <input type="checkbox" id="menu-control"/>
    <label class="menu-mobile  float-right " for="menu-control">
      <span class="btn-mobile  float-right ">&#9776;</span>
      <ul class="navigation-list">
        
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="https://bakanaouji.github.io/about">About</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="https://bakanaouji.github.io/works">Works</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="https://qiita.com/bakanaouji">Blog</a>
            </li>
          
        
        
      </ul>
    </label>
  </section>
</nav>


      <div class="content">
        
  <section class="container page">
  <article>
    <header>
      <h1></h1>
    </header>

    <h1 id="research">Research</h1>
<h5 id="international-conference">International Conference</h5>
<ol>
<li>Noboru Isobe, Kenshi Abe, Kaito Ariu.<br>
<strong>Last Iterate Convergence in Monotone Mean Field Games</strong><br>
NeurIPS 2025. [<a href="https://arxiv.org/abs/2410.05127">paper</a>]</li>
<li>Yuma Fujimoto, Kaito Ariu, Kenshi Abe.<br>
<strong>Learning from Delayed Feedback in Games via Extra Prediction</strong><br>
NeurIPS 2025. [<a href="https://arxiv.org/abs/2509.22426">paper</a>]</li>
<li>Yuma Fujimoto, Kaito Ariu, Kenshi Abe.<br>
<strong>Global Behavior of Learning Dynamics in Zero-Sum Games with Memory Asymmetry</strong><br>
AAMAS 2025 (Full paper). [<a href="https://arxiv.org/abs/2405.14546">paper</a>]</li>
<li>Yuma Fujimoto, Kaito Ariu, Kenshi Abe.<br>
<strong>Nash Equilibrium and Learning Dynamics in Three-Player Matching m-Action Games</strong><br>
AAMAS 2025 (Extended abstract). [<a href="https://arxiv.org/abs/2402.10825">paper</a>]</li>
<li>Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, Kenshi Abe.<br>
<strong>Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for Language Model Alignment</strong><br>
NAACL 2025. [<a href="https://arxiv.org/abs/2404.01054">paper</a>]</li>
<li>Kenshi Abe, Mitsuki Sakamoto, Kaito Ariu, Atsushi Iwasaki.<br>
<strong>Boosting Perturbed Gradient Ascent for Last-Iterate Convergence in Games</strong><br>
ICLR 2025. [<a href="https://arxiv.org/abs/2410.02388">paper</a>]</li>
<li>Daiki Katsuragawa, Yusuke Kaneko, Kaito Ariu, Kenshi Abe.<br>
<strong>Efficient Creative Selection in Online Advertising using Top-Two Thompson Sampling</strong><br>
WSDM 2025 (Industry day talks). [<a href="https://www.researchgate.net/publication/388836603_Efficient_Creative_Selection_in_Online_Advertising_using_Top-Two_Thompson_Sampling">paper</a>]</li>
<li>Hiroki Ishibashi, Kenshi Abe, Atsushi Iwasaki.<br>
<strong>Approximate State Abstraction for Markov Games</strong><br>
AAAI 2025. [<a href="https://arxiv.org/abs/2412.15877">paper</a>]</li>
<li>Yuma Fujimoto, Kaito Ariu, Kenshi Abe.<br>
<strong>Synchronization behind Learning in Periodic Zero-Sum Games Triggers Divergence from Nash equilibrium</strong><br>
AAAI 2025. [<a href="https://arxiv.org/abs/2408.10595">paper</a>]</li>
<li>Tetsuro Morimura, Mitsuki Sakamoto, Yuu Jinnai, Kenshi Abe, Kaito Ariu.<br>
<strong>Filtered Direct Preference Optimization</strong><br>
EMNLP 2024. [<a href="https://arxiv.org/abs/2404.13846">paper</a>]</li>
<li>Tetsuro Morimura, Kazuhiro Ota, Kenshi Abe, Peinan Zhang.<br>
<strong>Policy Gradient Algorithms with Monte-Carlo Tree Search for Non-Markov Decision Processes</strong><br>
Reinforcement Learning Conference (RLC) 2024. [<a href="https://arxiv.org/abs/2206.01011">paper</a>]</li>
<li>Kenshi Abe, Kaito Ariu, Mitsuki Sakamoto, Atsushi Iwasaki.<br>
<strong>Adaptively Perturbed Mirror Descent for Learning in Games</strong><br>
ICML 2024. [<a href="https://arxiv.org/abs/2305.16610">paper</a>]</li>
<li>Yuu Jinnai, Tetsuro Morimura, Ukyo Honda, Kaito Ariu, Kenshi Abe.<br>
<strong>Model-Based Minimum Bayes Risk Decoding</strong><br>
ICML 2024. [<a href="https://arxiv.org/abs/2311.05263">paper</a>]</li>
<li>Riku Togashi, Kenshi Abe, Yuta Saito.<br>
<strong>Scalable and Provably Fair Exposure Control for Large-Scale Recommender Systems</strong><br>
WWW 2024. [<a href="https://arxiv.org/abs/2209.04394">paper</a>]</li>
<li>Hakuei Yamada, Junpei Komiyama, Kenshi Abe, Atsushi Iwasaki.<br>
<strong>Learning Fair Division from Bandit Feedback</strong><br>
AISTATS 2024. [<a href="https://arxiv.org/abs/2311.09068">Paper</a>]</li>
<li>Yuma Fujimoto, Kaito Ariu, Kenshi Abe.<br>
<strong>Memory Asymmetry Creates Heteroclinic Orbits to Nash Equilibrium in Learning in Zero-Sum Games</strong><br>
AAAI 2024. [<a href="https://arxiv.org/abs/2305.13619">paper</a>]</li>
<li>Hiroaki Shiino, Kaito Ariu, Kenshi Abe, Togashi Riku.<br>
<strong>Exploration of Unranked Items in Safe Online Learning to Re-Rank</strong><br>
SIGIR 2023 (Short Paper). [<a href="https://arxiv.org/abs/2305.01202">paper</a>]</li>
<li>Kenshi Abe, Kaito Ariu, Mitsuki Sakamoto, Kentaro Toyoshima, Atsushi Iwasaki.<br>
<strong>Last-Iterate Convergence with Full and Noisy Feedback in Two-Player Zero-Sum Games</strong><br>
AISTATS 2023. [<a href="https://arxiv.org/abs/2208.09855">paper</a>]</li>
<li>Yuma Fujimoto, Kaito Ariu, Kenshi Abe.<br>
<strong>Learning in Multi-Memory Games Triggers Complex Dynamics Diverging from Nash Equilibrium</strong><br>
IJCAI 2023. [<a href="https://arxiv.org/abs/2302.01073">paper</a>]</li>
<li>Kenshi Abe, Mitsuki Sakamoto, Atsushi Iwasaki.<br>
<strong>Mutation-Driven Follow the Regularized Leader for Last-Iterate Convergence in Zero-Sum Games</strong><br>
UAI 2022. [<a href="https://arxiv.org/abs/2206.09254">paper</a>]</li>
<li>Kaito Ariu, Kenshi Abe, Alexandre Proutière.<br>
<strong>Thresholded LASSO Bandit</strong><br>
ICML 2022. [<a href="https://arxiv.org/abs/2010.11994">paper</a>]</li>
<li>Kenshi Abe, Junpei Komiyama, Atsushi Iwasaki.<br>
<strong>Anytime Capacity Expansion in Medical Residency Match by Monte Carlo Tree Search</strong><br>
IJCAI 2022. [<a href="https://arxiv.org/abs/2202.06570">paper</a>]</li>
<li>Kenshi Abe, Yusuke Kaneko.<br>
<strong>Off-Policy Exploitability-Evaluation in Two-Player Zero-Sum Markov Games</strong><br>
AAMAS 2021 (Full Paper). [<a href="https://arxiv.org/abs/2007.02141">paper</a>]</li>
</ol>
<h5 id="international-workshop">International Workshop</h5>
<ol>
<li>Kaito Ariu, Po-An Wang, Alexandre Proutiere, Kenshi Abe.<br>
<strong>Policy Testing in Markov Decision Processes</strong><br>
NeurIPS 2025 Workshop on Aligning Reinforcement Learning Experimentalists and Theorists. [<a href="https://arxiv.org/abs/2505.15342">paper</a>]</li>
<li>Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, Kenshi Abe.<br>
<strong>Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment</strong><br>
ICML 2024 Workshop on Models of Human Feedback for AI Alignment. [<a href="https://arxiv.org/abs/2404.01054">paper</a>]</li>
<li>Tetsuro Morimura, Mitsuki Sakamoto, Yuu Jinnai, Kenshi Abe, Kaito Ariu.<br>
<strong>Filtered Direct Preference Optimization</strong><br>
ICML 2024 Workshop on Models of Human Feedback for AI Alignment. [<a href="https://arxiv.org/abs/2404.13846">paper</a>]</li>
<li>Riku Togashi, Kenshi Abe.<br>
<strong>Fair Matrix Factorisation for Large-Scale Recommender Systems</strong><br>
RecSys 2022 FAccTRec Workshop. [<a href="https://arxiv.org/abs/2209.04394">paper</a>]</li>
<li>Yuki Shimano, Kenshi Abe, Atsushi Iwasaki, Kazunori Ohkawara.<br>
<strong>Computing Strategies of American Football via Counterfactual Regret Minimization</strong><br>
AAAI 2022 Workshop on Reinforcement Learning in Games (Oral Presentation). [<a href="http://aaai-rlg.mlanctot.info/papers/AAAI22-RLG_paper_23.pdf">paper</a>]</li>
<li>Masahiro Kato, Kei Nakagawa, Kenshi Abe, Tetsuro Morimura.<br>
<strong>Direct Expected Quadratic Utility Maximization for Mean-Variance Controlled Reinforcement Learning</strong><br>
NeurIPS 2021 Workshop on Deep Reinforcement Learning. [<a href="https://arxiv.org/abs/2010.01404">paper</a>]</li>
<li>Gota Morishita<sup>*</sup>, Kenshi Abe<sup>*</sup>, Kazuhisa Ogawa, Yusuke Kaneko (<sup>*</sup>equal contribution).<br>
<strong>Online Learning for Bidding Agent in First Price Auction</strong><br>
AAAI 2020 Workshop on Reinforcement Learning in Games. [<a href="http://aaai-rlg.mlanctot.info/papers/AAAI20-RLG_paper_9.pdf">paper</a>]</li>
</ol>
<h5 id="journal">Journal</h5>
<ol>
<li>Tsunehiko Tanaka, Kenshi Abe, Kaito Ariu, Tetsuro Morimura, Edgar Simo-Serra.<br>
<strong>Return-Aligned Decision Transformer</strong><br>
Transactions on Machine Learning Research. [<a href="https://arxiv.org/abs/2402.03923">paper</a>]</li>
<li>Yuki Ichihara, Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, Kenshi Abe, Mitsuki Sakamoto, Eiji Uchibe.<br>
<strong>Evaluation of Best-of-N Sampling Strategies for Language Model Alignment</strong><br>
Transactions on Machine Learning Research. [<a href="https://arxiv.org/abs/2502.12668">paper</a>]</li>
<li>阿部 拳之, 豊島 健太郎, 坂本 充生, 岩崎 敦.<br>
<strong>二人零和ゲームにおける突然変異駆動型正則化先導者追従法の終極反復収束</strong><br>
情報処理学会論文誌. [<a href="https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&amp;page_id=13&amp;block_id=8&amp;item_id=234275&amp;item_no=1">paper</a>]</li>
</ol>
<h5 id="preprints">Preprints</h5>
<ol>
<li>Kenshi Abe, Mitsuki Sakamoto, Kaito Ariu, Atsushi Iwasaki.<br>
<strong>Asymmetric Perturbation in Solving Bilinear Saddle-Point Optimization</strong><br>
[<a href="https://arxiv.org/abs/2506.05747">Arxiv</a>]</li>
<li>Wataru Masaka, Mitsuki Sakamoto, Kenshi Abe, Kaito Ariu, Tuomas Sandholm, Atsushi Iwasaki.<br>
<strong>The Power of Perturbation under Sampling in Solving Extensive-Form Games</strong><br>
[<a href="https://arxiv.org/abs/2501.16600">Arxiv</a>]</li>
<li>Yuma Fujimoto, Kaito Ariu, Kenshi Abe.<br>
<strong>Time-Varyingness in Auction Breaks Revenue Equivalence</strong><br>
[<a href="https://arxiv.org/abs/2410.12306">Arxiv</a>]</li>
<li>Sho Shimoyama, Tetsuro Morimura, Kenshi Abe, Toda Takamichi, Yuta Tomomatsu, Masakazu Sugiyama, Asahi Hentona, Yuuki Azuma, Hirotaka Ninomiya.<br>
<strong>Why Guided Dialog Policy Learning performs well? Understanding the role of adversarial learning and its alternative</strong><br>
[<a href="https://arxiv.org/abs/2307.06721">Arxiv</a>]</li>
<li>Masahiro Kato, Kenshi Abe, Kaito Ariu, Shota Yasui.<br>
<strong>A Practical Guide of Off-Policy Evaluation for Bandit Problems</strong><br>
[<a href="https://arxiv.org/abs/2010.12470">Arxiv</a>]</li>
<li>Masahiro Nomura, Kenshi Abe.<br>
<strong>A Simple Heuristic for Bayesian Optimization with A Low Budget</strong><br>
[<a href="https://arxiv.org/abs/1911.07790">Arxiv</a>]</li>
</ol>
<h5 id="domestic-conference">Domestic Conference</h5>
<ol>
<li>眞坂 航宙, 坂本 充生, 阿部 拳之, 蟻生 開人, 岩崎 敦.<br>
不完全情報展開型ゲームの求解における利得摂動に関する研究.<br>
<a href="https://www.ipsj.or.jp/event/taikai/87/index.html">情報処理学会 第87回全国大会</a>.</li>
<li>坂本 充生, 陣内 佑, 森村 哲郎, 阿部 拳之, 蟻生 開人.<br>
大規模言語モデルのためのアライメントデータ合成手法の実験的評価.<br>
<a href="https://www.anlp.jp/nlp2025/">言語処理学会 第31回年次大会 (NLP 2025)</a> [<a href="https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/Q4-11.pdf">paper</a>].</li>
<li>森村 哲郎, 坂本 充生, 陣内 佑, 阿部 拳之, 蟻生 開人.<br>
ベイズリスク選好最適化：報酬モデル不要のオンライン選好最適化手法.<br>
<a href="https://ibisml.org/ibis2024/">第27回情報論的学習理論ワークショップ (IBIS 2024)</a>.</li>
<li>眞坂 航宙, 坂本 充生, 阿部 拳之, 蟻生 開人, 岩崎 敦.<br>
（不完全情報）展開型ゲームにおける零分散の利得摂動手法.<br>
<a href="https://ibisml.org/ibis2024/">第27回情報論的学習理論ワークショップ (IBIS 2024)</a>.</li>
<li>市原 有生希, 陣内 佑, 森村 哲郎, 阿部 拳之, 蟻生 開人, 坂本 充生, 内部 英治.<br>
Evaluation of Best-of-N Sampling Strategies for Language Model Alignment.<br>
<a href="https://ibisml.org/ibis2024/">第27回情報論的学習理論ワークショップ (IBIS 2024)</a>.</li>
<li>磯部 伸, 阿部 拳之, 蟻生 開人.<br>
Last Iterate Convergence in Monotone Mean Field Games.<br>
<a href="https://ibisml.org/ibis2024/">第27回情報論的学習理論ワークショップ (IBIS 2024)</a>.</li>
<li>蟻生 開人, Po-An Wang, 阿部 拳之, Alexandre Proutiere.<br>
マルコフ決定過程における良方策検定手法の提案.<br>
<a href="https://ibisml.org/ibis2024/">第27回情報論的学習理論ワークショップ (IBIS 2024)</a>.</li>
<li>坂本 充生, 森村 哲郎, 陣内 佑, 阿部 拳之, 蟻生 開人.<br>
Filtered Direct Preference Optimization: 選好データセットの質に基づくフィルタリング手法の提案.<br>
<a href="https://ibisml.org/ibis2024/">第27回情報論的学習理論ワークショップ (IBIS 2024)</a>.</li>
<li>藤本 悠雅, 蟻生 開人, 阿部 拳之.<br>
Synchronization behind Learning in Periodic Zero-Sum Games Triggers Divergence from Nash equilibrium.<br>
<a href="https://ibisml.org/ibis2024/">第27回情報論的学習理論ワークショップ (IBIS 2024)</a>.</li>
<li>石橋 宙希, 阿部 拳之, 岩崎 敦.<br>
二人零和マルコフゲームにおける状態抽象化に関する研究.<br>
<a href="https://www.ipsj.or.jp/event/fit/fit2024/abstract/data/html/program/f.html">第23回情報科学技術フォーラム (FIT 2024)</a>.</li>
<li>坂本 充生, 阿部 拳之, 蟻生 開人, 岩崎 敦.<br>
RLHFにおける分布シフトの評価.<br>
<a href="https://www.ai-gakkai.or.jp/jsai2024/">第38回人工知能学会全国大会 (JSAI 2024)</a>. [<a href="https://www.jstage.jst.go.jp/article/pjsai/JSAI2024/0/JSAI2024_1B3GS202/_article/-char/ja/">paper</a>]</li>
<li>石橋 宙希, 島野 雄貴, 阿部 拳之, 岩崎 敦.<br>
二人零和マルコフゲームにおける状態抽象化法に関する研究.<br>
<a href="https://www.ipsj.or.jp/event/taikai/86/">情報処理学会 第86回全国大会</a> [<a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&amp;active_action=repository_view_main_item_detail&amp;item_id=235852&amp;item_no=1&amp;page_id=13&amp;block_id=8">paper</a>].</li>
<li>板垣 圭知, 小宮山 純平, 阿部 拳之, 岩崎 敦.<br>
研修医配属における地域間格差を調整する制約のモンテカルロ木探索.<br>
<a href="https://www.ipsj.or.jp/event/taikai/86/">情報処理学会 第86回全国大会</a> [<a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&amp;active_action=repository_view_main_item_detail&amp;item_id=235850&amp;item_no=1&amp;page_id=13&amp;block_id=8">paper</a>].</li>
<li>阿部 拳之, 蟻生 開人, 坂本 充生, 岩崎 敦.<br>
A Slingshot Approach to Learning in Monotone Games.<br>
<a href="https://ibisml.org/ibis2023/">第26回情報論的学習理論ワークショップ (IBIS 2023)</a>.</li>
<li>坂本 充生, 阿部 拳之, 蟻生 開人, 岩崎 敦.<br>
Zero-Variance Perturbation Utiity for Extensive-Form Games.<br>
<a href="https://ibisml.org/ibis2023/">第26回情報論的学習理論ワークショップ (IBIS 2023)</a>.</li>
<li>藤本 悠雅, 蟻生 開人, 阿部 拳之.<br>
Learning in Multi-Memory Games Triggers Complex Dynamics Diverging from Nash Equilibrium.<br>
<a href="https://ibisml.org/ibis2023/">第26回情報論的学習理論ワークショップ (IBIS 2023)</a>.</li>
<li>山田 博瑛, 小宮山 純平, 阿部 拳之, 岩崎 敦.<br>
オンライン環境において公平な資源配分を実現するアルゴリズムに関する研究.<br>
<a href="https://ibisml.org/ibis2023/">第26回情報論的学習理論ワークショップ (IBIS 2023)</a>.</li>
<li>山田 博瑛, 小宮山 純平, 阿部 拳之, 岩崎 敦.<br>
オンライン環境において公平な資源配分を実現するアルゴリズムに関する研究.<br>
<a href="https://www.ipsj.or.jp/event/fit/fit2023/abstract/data/html/program/f.html">第22回情報科学技術フォーラム (FIT 2023)</a>.</li>
<li>板垣 圭知, 小宮山 純平, 阿部 拳之, 岩崎 敦.<br>
研修医配属における地域間格差を調整するための制約のモンテカルロ木探索.<br>
<a href="https://www.ipsj.or.jp/event/fit/fit2023/abstract/data/html/program/f.html">第22回情報科学技術フォーラム (FIT 2023)</a>.</li>
<li>坂本 充生, 阿部 拳之, 蟻生 開人, 岩崎 敦.<br>
二人零和展開型ゲームにおける突然変異付き乗算型重み更新に関する研究.<br>
<a href="https://www.ai-gakkai.or.jp/jsai2023/">第37回人工知能学会全国大会 (JSAI 2023)</a>. [<a href="https://www.jstage.jst.go.jp/article/pjsai/JSAI2023/0/JSAI2023_2T4GS502/_article/-char/ja/">paper</a>]</li>
<li>山田 博瑛, 小宮山 純平, 阿部 拳之, 岩崎 敦.<br>
オンライン環境において公平な資源配分を実現するアルゴリズムに関する研究.<br>
<a href="https://www.ai-gakkai.or.jp/jsai2023/">第37回人工知能学会全国大会 (JSAI 2023)</a>. [<a href="https://www.jstage.jst.go.jp/article/pjsai/JSAI2023/0/JSAI2023_2T4GS503/_article/-char/ja/">paper</a>]</li>
<li>戸田 隆道, 森村 哲郎, 阿部 拳之.<br>
タスク指向対話システムの方策学習への Decision Transformerの適用.<br>
<a href="https://www.anlp.jp/nlp2023/">言語処理学会 第29回年次大会 (NLP 2023)</a> [<a href="https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_dir/Q12-6.pdf">paper</a>].</li>
<li>下山 翔, 森村 哲郎, 阿部 拳之.<br>
タスク指向対話における強化学習を用いた対話方策学習への敵対的学習の役割の解明.<br>
<a href="https://www.anlp.jp/nlp2023/">言語処理学会 第29回年次大会 (NLP 2023)</a> [<a href="https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_dir/Q12-5.pdf">paper</a>].</li>
<li>山田 博瑛, 小宮山 純平, 阿部 拳之, 岩崎 敦.<br>
オンライン環境において公平な資源配分を実現するアルゴリズムに関する研究.<br>
<a href="https://www.ipsj.or.jp/event/taikai/85/">情報処理学会 第85回全国大会</a> [<a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&amp;active_action=repository_view_main_item_detail&amp;item_id=230043&amp;item_no=1&amp;page_id=13&amp;block_id=8">paper</a>].</li>
<li>板垣 圭知, 小宮山 純平, 阿部 拳之, 岩崎 敦.<br>
研修医配属における地域間格差を調整するための制約のモンテカルロ木探索.<br>
<a href="https://www.ipsj.or.jp/event/taikai/85/">情報処理学会 第85回全国大会</a> [<a href="https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&amp;page_id=13&amp;block_id=8&amp;item_id=230050&amp;item_no=1">paper</a>].</li>
<li>阿部 拳之, 蟻生 開人, 坂本 充生, 豊島 健太郎, 岩崎 敦.<br>
Last-Iterate Convergence with Full- and Noisy-Information Feedback in Two-Player Zero-Sum Games.<br>
<a href="https://ibisml.org/ibis2022/">第25回情報論的学習理論ワークショップ (IBIS 2022)</a>.</li>
<li>蟻生 開人, 阿部 拳之, Alexandre Proutiere.<br>
Thresholded Lasso Bandit.<br>
<a href="https://ibisml.org/ibis2022/">第25回情報論的学習理論ワークショップ (IBIS 2022)</a>.</li>
<li>富樫 陸, 阿部 拳之.<br>
公平性を考慮した大規模推薦システム.<br>
<a href="https://ibisml.org/ibis2022/">第25回情報論的学習理論ワークショップ (IBIS 2022)</a>.</li>
<li>森村 哲郎, 大田 和寛, 阿部 拳之, 張 培楠.<br>
ビームサーチ推論のための強化学習.<br>
<a href="https://ibisml.org/ibis2022/">第25回情報論的学習理論ワークショップ (IBIS 2022)</a>.</li>
<li>豊島 健太郎, 坂本 充生, 阿部 拳之, 岩崎 敦.<br>
二人零和ゲームにおける突然変異駆動型Follow-The-Regularized-Leaderの終極反復収束.<br>
<a href="https://www.ipsj.or.jp/event/fit/fit2022/FIT2022_program/data/html/program/f.html">第21回情報科学技術フォーラム (FIT 2022)</a>.</li>
<li>坂本 充生, 豊島 健太郎, 阿部 拳之, 岩崎 敦.<br>
二人零和ゲームにおける突然変異付きレプリケータダイナミクスを用いた学習アルゴリズムに関する研究.<br>
<a href="https://www.ai-gakkai.or.jp/jsai2022/">第36回人工知能学会全国大会 (JSAI 2022)</a>. [<a href="https://www.jstage.jst.go.jp/article/pjsai/JSAI2022/0/JSAI2022_2O6GS502/_pdf/-char/ja">paper</a>]</li>
<li>坂本 充生, 阿部 拳之, 岩崎 敦.<br>
二人零和ゲームにおける突然変異付きレプリケータダイナミクスを用いた学習アルゴリズムに関する研究.<br>
<a href="https://www.ipsj.or.jp/event/taikai/84/index.html">情報処理学会 第84回全国大会</a>.</li>
<li>豊島 健太郎, 坂本 充生, 阿部 拳之, 岩崎 敦.<br>
クールノー競争におけるマルチエージェント強化学習に関する研究.<br>
<a href="https://www.ipsj.or.jp/event/taikai/84/index.html">情報処理学会 第84回全国大会</a>.</li>
<li>坂本 充生, 阿部 拳之, 岩崎 敦.<br>
見間違えのある繰り返しゲームのためのActor-Critic型強化学習.<br>
<a href="https://ibisml.org/ibis2021/">第24回情報論的学習理論ワークショップ (IBIS 2021)</a>.</li>
<li>坂本 充生, 阿部 拳之, 岩崎 敦.<br>
見間違えのある繰り返しゲームのためのActor-Critic型強化学習.<br>
<a href="https://orsj.org/nc2021f/">日本オペレーションズ・リサーチ学会 2021年 秋季研究発表会</a>. [<a href="https://orsj.org/nc2021f/wp-content/uploads/sites/2/2021/08/2021f-2-C-11.pdf">paper</a>]</li>
<li>坂本 充生, 阿部 拳之, 岩崎 敦.<br>
見間違えのある繰り返し囚人のジレンマにおける方策勾配法に関する研究.<br>
<a href="https://www.ipsj.or.jp/event/fit/fit2021/FIT2021_program/data/html/program/f.html">第20回情報科学技術フォーラム (FIT 2021)</a> (FIT船井ベストペーパー賞). [<a href="https://www.ipsj.or.jp/award/9faeag0000004eyo-att/CF-002.pdf">paper</a>]</li>
<li>島野 雄貴, 阿部 拳之, 岩崎 敦, 大河原 一憲.<br>
反実仮想後悔最小化によるアメリカンフットボールにおけるオフェンス戦略の均衡推定.<br>
<a href="https://www.ipsj.or.jp/event/fit/fit2021/FIT2021_program/data/html/program/f.html">第20回情報科学技術フォーラム (FIT 2021)</a>.</li>
<li>阿部 拳之, 金子 雄祐.<br>
二人零和マルコフゲームにおけるオフ方策評価のためのQ学習.<br>
<a href="https://www.logos.ic.i.u-tokyo.ac.jp/~tsuruoka/sig-gi/gpw/2020/index.html">第25回ゲームプログラミングワークショップ (GPW 2020)</a>. <a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&amp;active_action=repository_view_main_item_detail&amp;item_id=207671&amp;item_no=1&amp;page_id=13&amp;block_id=8">[paper]</a></li>
<li>阿部 拳之.<br>
花札におけるナッシュ均衡戦略の計算.<br>
<a href="http://ibisml.org/ibis2019/">第22回情報論的学習理論ワークショップ (IBIS 2019)</a>.</li>
<li>野村 将寛, 阿部 拳之.<br>
Black-box最適化に対するBudgetを考慮した探索空間の初期化.<br>
<a href="https://www.ai-gakkai.or.jp/jsai2019/">第33回人工知能学会全国大会 (JSAI 2019)</a>. [<a href="https://www.jstage.jst.go.jp/article/pjsai/JSAI2019/0/JSAI2019_4Rin102/_article/-char/ja">paper</a>]</li>
<li>阿部 拳之, 野村 将寛.<br>
非定常多腕バンディットアルゴリズムを用いたハイパーパラメータ最適化フレームワークの提案.<br>
<a href="http://ibisml.org/ibis2018/">第21回情報論的学習理論ワークショップ (IBIS 2018)</a>.</li>
<li>阿部 拳之, 小野 功.<br>
活用と探索の釣り合いを考慮した事例ベース政策最適化.<br>
<a href="http://www.jpnsec.org/symposium201701.html">第12回進化計算学会研究会 (2017年)</a>. (ベストポスター発表賞)</li>
<li>阿部 拳之, 小野 功.<br>
多峰性景観下での自然進化戦略による事例ベース政策最適化.<br>
<a href="https://www.sice.or.jp/org/SSI2016/">計測自動制御学会システム・情報部門学術講演会 (SSI 2016)</a>.</li>
<li>阿部 拳之, 小野 功.<br>
自然進化戦略を用いた事例ベース政策最適化.<br>
<a href="https://www.sice.or.jp/system/system_ken54.html">第54回システム工学部会研究会 (2016年)</a>.</li>
</ol>
<h1 id="presentations">Presentations</h1>
<ol>
<li><a href="https://speakerdeck.com/bakanaouji/bu-wan-quan-qing-bao-kemunojun-heng-xue-xi">学習理論におけるゲーム理論のフロンティア</a><br>
<a href="https://www.ai-gakkai.or.jp/jsai2025/ks#ks-12">第39回人工知能学会全国大会 (JSAI 2025) 企画セッション</a> (2025/5/28).</li>
<li><strong>Adaptively Perturbed Mirror Descent for Learning in Games</strong><br>
<a href="https://cyberagent.connpass.com/event/320641/">ICML 2024 著者発表会</a> (2024/7/10).</li>
<li>機械学習が紡ぐゲーム理論のフロンティア<br>
<a href="https://www.ai-gakkai.or.jp/jsai2024/ks#ks-1">第38回人工知能学会全国大会 (JSAI 2024) 企画セッション</a> (2024/5/29).</li>
<li><a href="https://speakerdeck.com/bakanaouji/learning-in-games">Learning in games: ゲーム理論とオンライン学習</a><br>
<a href="https://cyberagent.connpass.com/event/286764/">Algorithmic Learning &amp; Optimization 勉強会#1</a> (2023/6/24).</li>
<li><a href="https://www.slideshare.net/KenshiAbe/ss-248654457">二人零和マルコフゲームにおけるオフ方策評価</a><br>
<a href="https://connpass.com/event/212654/">AAMAS2021 著者発表会</a> (2021/5/27).</li>
<li>広告配信オークションにおける入札戦略<br>
<a href="https://www.ipsj.or.jp/event/fit/fit2020/splist-AITECHTALK.html">第19回情報科学技術フォーラム (FIT 2020)</a> (2020/9/1).</li>
<li>多人数不完全情報ゲームにおけるAI開発<br>
<a href="https://www.jeameetings.org/2020s/index.html">日本経済学会 2020年度春季大会</a> (2020/5/30).</li>
<li><a href="https://www.slideshare.net/KenshiAbe/ai-165308197">多人数不完全情報ゲームにおけるAI ~ポーカーと麻雀を例として~</a><br>
<a href="https://rlarch.connpass.com/event/143128/">第43回強化学習アーキテクチャ勉強会</a> (2019/8/20).</li>
</ol>
<h1 id="blog-posts">Blog Posts</h1>
<ul>
<li><a href="https://qiita.com/bakanaouji/items/f70d7948931c96d94ef8">【ゲーム理論】展開型ゲームのナッシュ均衡を計算しよう：Counterfactual Regret Minimizationの解説</a></li>
<li><a href="https://qiita.com/bakanaouji/items/fefa93cc53cafbdd985d">【Unity ML-Agents】 Self-Play Reinforcement Learningで対戦ゲームのAIを作ってみた</a></li>
<li><a href="https://qiita.com/bakanaouji/items/d20c8903a1327e660de5">Q-Learningがどの程度Off-Policyなのかを調べてみた</a></li>
<li><a href="https://cyberagent.ai/blog/research/2522/">ミニ花札のAIを作ってみよう</a></li>
<li><a href="https://qiita.com/bakanaouji/items/aa076cef1e04f77f48ce">遺伝的アルゴリズムでコードフォーマッタのスタイルを最適化する</a></li>
<li><a href="https://qiita.com/bakanaouji/items/75444b4d97ede83c7c48">max k-armed banditとは？</a></li>
<li><a href="https://cyberagent.ai/blog/research/1036">Successive Halvingの性能解析</a></li>
</ul>

  </article>
</section>


      </div>
      
        <footer class="footer">
  <section class="container">
    
      <div class="sns-shares sp-sns-shares">
        
        
        
        
        
      </div>
    
    
     © 0001    ·  Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/naro143/hugo-coder-portfolio">CoderPortfolio</a>. 

  </section>
</footer>
<div class="fixed-bar">
  <section class="container">
    
      <p id="privateTriggerText">Do you want to get in touch with me? →<a id="privateTrigger">Click!</a></p>
    
    
      <div class="sns-shares pc-sns-shares">
        
        
        
        
        
      </div>
    
  </section>
</div>

      
    </main>

    
<script>
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-150920247-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

  <script src="/js/app.js"></script>
  
  <script>
  (function($) {
    $(function() {
      $('#privateTrigger').on('click', function() {
        $('.private').slideToggle();
        $('#privateTriggerText').text("Thank You! Please share it if you like it→");
      });
    });
   })(jQuery);
  </script>
  
  </body>
</html>
