<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Kenshi Abe">
    <meta name="description" content="https://bakanaouji.github.io/">
    <meta name="keywords" content="portfolio,researcher,personal">

    <meta property="og:site_name" content="Kenshi Abe">
    <meta property="og:title" content="
   - Kenshi Abe
">
    <meta property="og:description" content="">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://bakanaouji.github.io/works/">
    <meta property="og:image" content="https://bakanaouji.github.io/&lt;no value&gt;">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="https://bakanaouji.github.io/works/">
    <meta name="twitter:image" content="https://bakanaouji.github.io/&lt;no value&gt;">

    <base href="https://bakanaouji.github.io/works/">
    <title>
   - Kenshi Abe
</title>

    <link rel="canonical" href="https://bakanaouji.github.io/works/">
    
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
    
    <link  rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700">
    <link rel="stylesheet" href="/css/normalize.min.css">
    <link rel="stylesheet" href="/css/style.min.css">

    

    
      <link rel="stylesheet" href="https://bakanaouji.github.io/custom.css">
    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    
      <link rel="alternate" href="https://bakanaouji.github.io/index.xml" type="application/rss+xml" title="Kenshi Abe">
      <link href="https://bakanaouji.github.io/index.xml" rel="feed" type="application/rss+xml" title="Kenshi Abe" />
    

    <meta name="generator" content="Hugo 0.110.0">
  </head>

  <body class="">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">Kenshi Abe</a>
    <input type="checkbox" id="menu-control"/>
    <label class="menu-mobile  float-right " for="menu-control">
      <span class="btn-mobile  float-right ">&#9776;</span>
      <ul class="navigation-list">
        
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="https://bakanaouji.github.io/about">About</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="https://bakanaouji.github.io/works">Works</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="https://qiita.com/bakanaouji">Blog</a>
            </li>
          
        
        
      </ul>
    </label>
  </section>
</nav>


      <div class="content">
        
  <section class="container page">
  <article>
    <header>
      <h1></h1>
    </header>

    <h1 id="research">Research</h1>
<h5 id="international-conference">International Conference</h5>
<ol>
<li>Kenshi Abe, Kaito Ariu, Mitsuki Sakamoto, Kentaro Toyoshima, Atsushi Iwasaki.<br>
<strong>Last-Iterate Convergence with Full- and Noisy-Information Feedback in Two-Player Zero-Sum Games</strong><br>
AISTATS 2023. [<a href="https://arxiv.org/abs/2208.09855">paper</a>].</li>
<li>Riku Togashi, Kenshi Abe.<br>
<strong>Fair Matrix Factorisation for Large-Scale Recommender Systems</strong><br>
RecSys 2022 FAccTRec Workshop. [<a href="https://arxiv.org/abs/2209.04394">paper</a>]</li>
<li>Kenshi Abe, Mitsuki Sakamoto, Atsushi Iwasaki.<br>
<strong>Mutation-Driven Follow the Regularized Leader for Last-Iterate Convergence in Zero-Sum Games</strong><br>
UAI 2022. [<a href="https://arxiv.org/abs/2206.09254">paper</a>]</li>
<li>Kaito Ariu, Kenshi Abe, Alexandre Proutière.<br>
<strong>Thresholded LASSO Bandit</strong><br>
ICML 2022. [<a href="https://arxiv.org/abs/2010.11994">paper</a>]</li>
<li>Kenshi Abe, Junpei Komiyama, Atsushi Iwasaki.<br>
<strong>Anytime Capacity Expansion in Medical Residency Match by Monte Carlo Tree Search</strong><br>
IJCAI 2022. [<a href="https://arxiv.org/abs/2202.06570">paper</a>]</li>
<li>Yuki Shimano, Kenshi Abe, Atsushi Iwasaki, Kazunori Ohkawara.<br>
<strong>Computing Strategies of American Football via Counterfactual Regret Minimization</strong><br>
AAAI 2022 Workshop on Reinforcement Learning in Games (Oral Presentation). [<a href="http://aaai-rlg.mlanctot.info/papers/AAAI22-RLG_paper_23.pdf">paper</a>]</li>
<li>Masahiro Kato, Kei Nakagawa, Kenshi Abe, Tetsuro Morimura.<br>
<strong>Direct Expected Quadratic Utility Maximization for Mean-Variance Controlled Reinforcement Learning</strong><br>
NeurIPS 2021 Workshop on Deep Reinforcement Learning. [<a href="https://arxiv.org/abs/2010.01404">paper</a>]</li>
<li>Kenshi Abe, Yusuke Kaneko.<br>
<strong>Off-Policy Exploitability-Evaluation in Two-Player Zero-Sum Markov Games</strong><br>
AAMAS 2021 (Full Paper). [<a href="https://arxiv.org/abs/2007.02141">paper</a>]</li>
<li>Gota Morishita<sup>*</sup>, Kenshi Abe<sup>*</sup>, Kazuhisa Ogawa, Yusuke Kaneko (<sup>*</sup>equal contribution).<br>
<strong>Online Learning for Bidding Agent in First Price Auction</strong><br>
AAAI 2020 Workshop on Reinforcement Learning in Games. [<a href="http://aaai-rlg.mlanctot.info/papers/AAAI20-RLG_paper_9.pdf">paper</a>].</li>
</ol>
<h5 id="preprints">Preprints</h5>
<ol>
<li>Tetsuro Morimura, Kazuhiro Ota, Kenshi Abe, Peinan Zhang.<br>
<strong>Policy Gradient Algorithms with Monte-Carlo Tree Search for Non-Markov Decision Processes</strong><br>
[<a href="https://arxiv.org/abs/2206.01011">Arxiv</a>]</li>
<li>Masahiro Kato, Kenshi Abe, Kaito Ariu, Shota Yasui.<br>
<strong>A Practical Guide of Off-Policy Evaluation for Bandit Problems</strong><br>
[<a href="https://arxiv.org/abs/2010.12470">Arxiv</a>].</li>
<li>Masahiro Nomura, Kenshi Abe.<br>
<strong>A Simple Heuristic for Bayesian Optimization with A Low Budget</strong>.<br>
[<a href="https://arxiv.org/abs/1911.07790">Arxiv</a>].</li>
</ol>
<h5 id="internal-conference">Internal Conference</h5>
<ol>
<li>豊島健太郎, 坂本充生, 阿部拳之, 岩崎敦.<br>
二人零和ゲームにおける突然変異駆動型Follow-The-Regularized-Leaderの終極反復収束.<br>
<a href="https://onsite.gakkai-web.net/fit2022/abstract/data/html/program/f.html">第21回情報科学技術フォーラム (FIT 2022)</a>.</li>
<li>坂本充生, 豊島健太郎, 阿部拳之, 岩崎敦.<br>
二人零和ゲームにおける突然変異付きレプリケータダイナミクスを用いた学習アルゴリズムに関する研究.<br>
<a href="https://www.ai-gakkai.or.jp/jsai2022/">第36回人工知能学会全国大会 (JSAI2022)</a>. [<a href="https://www.jstage.jst.go.jp/article/pjsai/JSAI2022/0/JSAI2022_2O6GS502/_pdf/-char/ja">paper</a>]</li>
<li>坂本充生, 阿部拳之, 岩崎敦.<br>
二人零和ゲームにおける突然変異付きレプリケータダイナミクスを用いた学習アルゴリズムに関する研究.<br>
<a href="https://www.ipsj.or.jp/event/taikai/84/index.html">情報処理学会 第84回全国大会</a>.</li>
<li>豊島健太郎, 坂本充生, 阿部拳之, 岩崎敦.<br>
クールノー競争におけるマルチエージェント強化学習に関する研究.<br>
<a href="https://www.ipsj.or.jp/event/taikai/84/index.html">情報処理学会 第84回全国大会</a>.</li>
<li>坂本充生, 阿部拳之, 岩崎敦.<br>
見間違えのある繰り返しゲームのためのActor-Critic型強化学習.<br>
<a href="https://ibisml.org/ibis2021/">日本オペレーションズ・リサーチ学会 2021年 秋季研究発表会</a>. [<a href="https://orsj.org/nc2021f/wp-content/uploads/sites/2/2021/08/2021f-2-C-11.pdf">paper</a>]</li>
<li>坂本充生, 阿部拳之, 岩崎敦.<br>
見間違えのある繰り返しゲームのためのActor-Critic型強化学習.<br>
<a href="https://ibisml.org/ibis2021/">第24回情報論的学習理論ワークショップ (IBIS2021)</a>.</li>
<li>島野雄貴, 阿部拳之, 岩崎敦, 大河原一憲.<br>
反実仮想後悔最小化によるアメリカンフットボールにおけるオフェンス戦略の均衡推定.<br>
<a href="https://www.ipsj.or.jp/event/fit/fit2021/FIT2021_program/data/html/program/f.html">第20回情報科学技術フォーラム (FIT 2021)</a>.</li>
<li>坂本充生, 阿部拳之, 岩崎敦.<br>
見間違えのある繰り返し囚人のジレンマにおける方策勾配法に関する研究.<br>
<a href="https://www.ipsj.or.jp/event/fit/fit2021/FIT2021_program/data/html/program/f.html">第20回情報科学技術フォーラム (FIT 2021)</a> (FIT船井ベストペーパー賞). [<a href="https://www.ipsj.or.jp/award/9faeag0000004eyo-att/CF-002.pdf">paper</a>]</li>
<li>阿部拳之, 金子雄祐.<br>
二人零和マルコフゲームにおけるオフ方策評価のためのQ学習.<br>
<a href="https://www.logos.ic.i.u-tokyo.ac.jp/~tsuruoka/sig-gi/gpw/2020/index.html">第25回ゲームプログラミングワークショップ (GPW 2020)</a>. <a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&amp;active_action=repository_view_main_item_detail&amp;item_id=207671&amp;item_no=1&amp;page_id=13&amp;block_id=8">[paper]</a></li>
<li>阿部拳之.<br>
広告配信オークションにおける入札戦略.<br>
<a href="https://www.ipsj.or.jp/event/fit/fit2020/splist-AITECHTALK.html">第19回情報科学技術フォーラム (FIT 2020)</a>.</li>
<li>阿部拳之.<br>
多人数不完全情報ゲームにおけるAI開発.<br>
<a href="https://www.jeameetings.org/2020s/index.html">日本経済学会 2020年度春季大会</a>.</li>
<li>阿部拳之.<br>
花札におけるナッシュ均衡戦略の計算.<br>
<a href="http://ibisml.org/ibis2019/">第22回情報論的学習理論ワークショップ (IBIS2019)</a>.</li>
<li>野村将寛, 阿部拳之.<br>
Black-box最適化に対するBudgetを考慮した探索空間の初期化.<br>
<a href="https://www.ai-gakkai.or.jp/jsai2019/">第33回人工知能学会全国大会 (JSAI 2019)</a>. [<a href="https://www.jstage.jst.go.jp/article/pjsai/JSAI2019/0/JSAI2019_4Rin102/_article/-char/ja">paper</a>]</li>
<li>阿部拳之, 野村将寛.<br>
非定常多腕バンディットアルゴリズムを用いたハイパーパラメータ最適化フレームワークの提案.<br>
<a href="http://ibisml.org/ibis2018/">第21回情報論的学習理論ワークショップ (IBIS2018)</a>.</li>
<li>阿部拳之, 小野功.<br>
活用と探索の釣り合いを考慮した事例ベース政策最適化.<br>
<a href="http://www.jpnsec.org/symposium201701.html">第12回進化計算学会研究会 (2017年)</a>. (ベストポスター発表賞)</li>
<li>阿部拳之, 小野功.<br>
多峰性景観下での自然進化戦略による事例ベース政策最適化.<br>
<a href="https://www.sice.or.jp/org/SSI2016/">計測自動制御学会システム・情報部門学術講演会 (SSI2016)</a>.</li>
<li>阿部拳之, 小野功.<br>
自然進化戦略を用いた事例ベース政策最適化.<br>
<a href="https://www.sice.or.jp/system/system_ken54.html">第54回システム工学部会研究会 (2016年)</a>.</li>
</ol>
<h1 id="presentations">Presentations</h1>
<ul>
<li><a href="https://www.slideshare.net/KenshiAbe/ss-248654457">二人零和マルコフゲームにおけるオフ方策評価</a></li>
<li><a href="https://www.slideshare.net/KenshiAbe/ai-165308197">多人数不完全情報ゲームにおけるAI ~ポーカーと麻雀を例として~</a></li>
<li>論文勉強会
<ul>
<li><a href="https://www.slideshare.net/KenshiAbe/deep-counterfactual-regret-minimization">Deep Counterfactual Regret Minimization</a></li>
<li><a href="https://www.slideshare.net/KenshiAbe/competitive-multiagent-inverse-reinforcement-learning-with-suboptimal-demonstrations-153126367">Competitive Multi-agent Inverse Reinforcement Learning with Sub-optimal Demonstrations</a></li>
<li><a href="https://www.slideshare.net/KenshiAbe/deep-qlearning-from-demonstrations">Deep Q-learning from Demonstrations</a></li>
<li><a href="https://www.slideshare.net/KenshiAbe/multiagent-reinforcement-learning-in-sequential-social-dilemmas-144749583">Multi-agent Reinforcement Learning in Sequential Social Dilemmas</a></li>
<li><a href="https://www.slideshare.net/KenshiAbe/evolved-policy-gradients">Evolved policy gradients</a></li>
</ul>
</li>
</ul>
<h1 id="blog-posts">Blog Posts</h1>
<ul>
<li><a href="https://qiita.com/bakanaouji/items/f70d7948931c96d94ef8">【ゲーム理論】展開型ゲームのナッシュ均衡を計算しよう：Counterfactual Regret Minimizationの解説</a></li>
<li><a href="https://qiita.com/bakanaouji/items/fefa93cc53cafbdd985d">【Unity ML-Agents】 Self-Play Reinforcement Learningで対戦ゲームのAIを作ってみた</a></li>
<li><a href="https://qiita.com/bakanaouji/items/d20c8903a1327e660de5">Q-Learningがどの程度Off-Policyなのかを調べてみた</a></li>
<li><a href="https://cyberagent.ai/blog/research/2522/">ミニ花札のAIを作ってみよう</a></li>
<li><a href="https://qiita.com/bakanaouji/items/aa076cef1e04f77f48ce">遺伝的アルゴリズムでコードフォーマッタのスタイルを最適化する</a></li>
<li><a href="https://qiita.com/bakanaouji/items/75444b4d97ede83c7c48">max k-armed banditとは？</a></li>
<li><a href="https://cyberagent.ai/blog/research/1036">Successive Halvingの性能解析</a></li>
</ul>

  </article>
</section>


      </div>
      
        <footer class="footer">
  <section class="container">
    
      <div class="sns-shares sp-sns-shares">
        
        
        
        
        
      </div>
    
    
     © 0001    ·  Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/naro143/hugo-coder-portfolio">CoderPortfolio</a>. 

  </section>
</footer>
<div class="fixed-bar">
  <section class="container">
    
      <p id="privateTriggerText">Do you want to get in touch with me? →<a id="privateTrigger">Click!</a></p>
    
    
      <div class="sns-shares pc-sns-shares">
        
        
        
        
        
      </div>
    
  </section>
</div>

      
    </main>

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-150920247-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

  <script src="/js/app.js"></script>
  
  <script>
  (function($) {
    $(function() {
      $('#privateTrigger').on('click', function() {
        $('.private').slideToggle();
        $('#privateTriggerText').text("Thank You! Please share it if you like it→");
      });
    });
   })(jQuery);
  </script>
  
  </body>
</html>
