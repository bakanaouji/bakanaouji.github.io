<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Kenshi Abe">
    <meta name="description" content="https://bakanaouji.github.io/">
    <meta name="keywords" content="portfolio,researcher,personal">

    <meta property="og:site_name" content="Kenshi Abe">
    <meta property="og:title" content="
   - Kenshi Abe
">
    <meta property="og:description" content="">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://bakanaouji.github.io/works/">
    <meta property="og:image" content="https://bakanaouji.github.io/&lt;no value&gt;">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="https://bakanaouji.github.io/works/">
    <meta name="twitter:image" content="https://bakanaouji.github.io/&lt;no value&gt;">

    <base href="https://bakanaouji.github.io/works/">
    <title>
   - Kenshi Abe
</title>

    <link rel="canonical" href="https://bakanaouji.github.io/works/">
    
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
    
    <link  rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700">
    <link rel="stylesheet" href="/css/normalize.min.css">
    <link rel="stylesheet" href="/css/style.min.css">

    

    
      <link rel="stylesheet" href="https://bakanaouji.github.io/custom.css">
    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    
      <link rel="alternate" href="https://bakanaouji.github.io/index.xml" type="application/rss+xml" title="Kenshi Abe">
      <link href="https://bakanaouji.github.io/index.xml" rel="feed" type="application/rss+xml" title="Kenshi Abe" />
    

    <meta name="generator" content="Hugo 0.121.1">
  </head>

  <body class="">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">Kenshi Abe</a>
    <input type="checkbox" id="menu-control"/>
    <label class="menu-mobile  float-right " for="menu-control">
      <span class="btn-mobile  float-right ">&#9776;</span>
      <ul class="navigation-list">
        
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="https://bakanaouji.github.io/about">About</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="https://bakanaouji.github.io/works">Works</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="https://qiita.com/bakanaouji">Blog</a>
            </li>
          
        
        
      </ul>
    </label>
  </section>
</nav>


      <div class="content">
        
  <section class="container page">
  <article>
    <header>
      <h1></h1>
    </header>

    <h1 id="research">Research</h1>
<h5 id="international-conference">International Conference</h5>
<ol>
<li>Kenshi Abe, Mitsuki Sakamoto, Kaito Ariu, Atsushi Iwasaki.<br>
<strong>Boosting Perturbed Gradient Ascent for Last-Iterate Convergence in Games</strong><br>
ICLR 2025. [<a href="https://arxiv.org/abs/2410.02388">paper</a>]</li>
<li>Yuma Fujimoto, Kaito Ariu, Kenshi Abe.<br>
<strong>Global Behavior of Learning Dynamics in Zero-Sum Games with Memory Asymmetry</strong><br>
AAMAS 2025 (Full paper). [<a href="https://arxiv.org/abs/2405.14546">paper</a>]</li>
<li>Yuma Fujimoto, Kaito Ariu, Kenshi Abe.<br>
<strong>Nash Equilibrium and Learning Dynamics in Three-Player Matching m-Action Games</strong><br>
AAMAS 2025 (Extended abstract). [<a href="https://arxiv.org/abs/2402.10825">paper</a>]</li>
<li>Hiroki Ishibashi, Kenshi Abe, Atsushi Iwasaki.<br>
<strong>Approximate State Abstraction for Markov Games</strong><br>
AAAI 2025. [<a href="https://arxiv.org/abs/2412.15877">paper</a>]</li>
<li>Yuma Fujimoto, Kaito Ariu, Kenshi Abe.<br>
<strong>Synchronization behind Learning in Periodic Zero-Sum Games Triggers Divergence from Nash equilibrium</strong><br>
AAAI 2025. [<a href="https://arxiv.org/abs/2408.10595">paper</a>]</li>
<li>Tetsuro Morimura, Mitsuki Sakamoto, Yuu Jinnai, Kenshi Abe, Kaito Ariu.<br>
<strong>Filtered Direct Preference Optimization</strong><br>
EMNLP 2024. [<a href="https://arxiv.org/abs/2404.13846">paper</a>]</li>
<li>Kenshi Abe, Kaito Ariu, Mitsuki Sakamoto, Atsushi Iwasaki.<br>
<strong>Adaptively Perturbed Mirror Descent for Learning in Games</strong><br>
ICML 2024. [<a href="https://arxiv.org/abs/2305.16610">paper</a>]</li>
<li>Yuu Jinnai, Tetsuro Morimura, Ukyo Honda, Kaito Ariu, Kenshi Abe.<br>
<strong>Model-Based Minimum Bayes Risk Decoding</strong><br>
ICML 2024. [<a href="https://arxiv.org/abs/2311.05263">paper</a>]</li>
<li>Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, Kenshi Abe.<br>
<strong>Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment</strong><br>
ICML 2024 Workshop on Models of Human Feedback for AI Alignment. [<a href="https://arxiv.org/abs/2404.01054">paper</a>]</li>
<li>Tetsuro Morimura, Mitsuki Sakamoto, Yuu Jinnai, Kenshi Abe, Kaito Ariu.<br>
<strong>Filtered Direct Preference Optimization</strong><br>
ICML 2024 Workshop on Models of Human Feedback for AI Alignment. [<a href="https://arxiv.org/abs/2404.13846">paper</a>]</li>
<li>Tetsuro Morimura, Kazuhiro Ota, Kenshi Abe, Peinan Zhang.<br>
<strong>Policy Gradient Algorithms with Monte-Carlo Tree Search for Non-Markov Decision Processes</strong><br>
Reinforcement Learning Conference (RLC) 2024. [<a href="https://arxiv.org/abs/2206.01011">paper</a>]</li>
<li>Riku Togashi, Kenshi Abe, Yuta Saito.<br>
<strong>Scalable and Provably Fair Exposure Control for Large-Scale Recommender Systems</strong><br>
WWW 2024. [<a href="https://arxiv.org/abs/2209.04394">paper</a>]</li>
<li>Hakuei Yamada, Junpei Komiyama, Kenshi Abe, Atsushi Iwasaki.<br>
<strong>Learning Fair Division from Bandit Feedback</strong><br>
AISTATS 2024. [<a href="https://arxiv.org/abs/2311.09068">Paper</a>]</li>
<li>Yuma Fujimoto, Kaito Ariu, Kenshi Abe.<br>
<strong>Memory Asymmetry Creates Heteroclinic Orbits to Nash Equilibrium in Learning in Zero-Sum Games</strong><br>
AAAI 2024. [<a href="https://arxiv.org/abs/2305.13619">paper</a>]</li>
<li>Hiroaki Shiino, Kaito Ariu, Kenshi Abe, Togashi Riku.<br>
<strong>Exploration of Unranked Items in Safe Online Learning to Re-Rank</strong><br>
SIGIR 2023 (Short Paper). [<a href="https://arxiv.org/abs/2305.01202">paper</a>]</li>
<li>Yuma Fujimoto, Kaito Ariu, Kenshi Abe.<br>
<strong>Learning in Multi-Memory Games Triggers Complex Dynamics Diverging from Nash Equilibrium</strong><br>
IJCAI 2023. [<a href="https://arxiv.org/abs/2302.01073">paper</a>]</li>
<li>Kenshi Abe, Kaito Ariu, Mitsuki Sakamoto, Kentaro Toyoshima, Atsushi Iwasaki.<br>
<strong>Last-Iterate Convergence with Full and Noisy Feedback in Two-Player Zero-Sum Games</strong><br>
AISTATS 2023. [<a href="https://arxiv.org/abs/2208.09855">paper</a>]</li>
<li>Riku Togashi, Kenshi Abe.<br>
<strong>Fair Matrix Factorisation for Large-Scale Recommender Systems</strong><br>
RecSys 2022 FAccTRec Workshop. [<a href="https://arxiv.org/abs/2209.04394">paper</a>]</li>
<li>Kenshi Abe, Mitsuki Sakamoto, Atsushi Iwasaki.<br>
<strong>Mutation-Driven Follow the Regularized Leader for Last-Iterate Convergence in Zero-Sum Games</strong><br>
UAI 2022. [<a href="https://arxiv.org/abs/2206.09254">paper</a>]</li>
<li>Kaito Ariu, Kenshi Abe, Alexandre Proutière.<br>
<strong>Thresholded LASSO Bandit</strong><br>
ICML 2022. [<a href="https://arxiv.org/abs/2010.11994">paper</a>]</li>
<li>Kenshi Abe, Junpei Komiyama, Atsushi Iwasaki.<br>
<strong>Anytime Capacity Expansion in Medical Residency Match by Monte Carlo Tree Search</strong><br>
IJCAI 2022. [<a href="https://arxiv.org/abs/2202.06570">paper</a>]</li>
<li>Yuki Shimano, Kenshi Abe, Atsushi Iwasaki, Kazunori Ohkawara.<br>
<strong>Computing Strategies of American Football via Counterfactual Regret Minimization</strong><br>
AAAI 2022 Workshop on Reinforcement Learning in Games (Oral Presentation). [<a href="http://aaai-rlg.mlanctot.info/papers/AAAI22-RLG_paper_23.pdf">paper</a>]</li>
<li>Masahiro Kato, Kei Nakagawa, Kenshi Abe, Tetsuro Morimura.<br>
<strong>Direct Expected Quadratic Utility Maximization for Mean-Variance Controlled Reinforcement Learning</strong><br>
NeurIPS 2021 Workshop on Deep Reinforcement Learning. [<a href="https://arxiv.org/abs/2010.01404">paper</a>]</li>
<li>Kenshi Abe, Yusuke Kaneko.<br>
<strong>Off-Policy Exploitability-Evaluation in Two-Player Zero-Sum Markov Games</strong><br>
AAMAS 2021 (Full Paper). [<a href="https://arxiv.org/abs/2007.02141">paper</a>]</li>
<li>Gota Morishita<sup>*</sup>, Kenshi Abe<sup>*</sup>, Kazuhisa Ogawa, Yusuke Kaneko (<sup>*</sup>equal contribution).<br>
<strong>Online Learning for Bidding Agent in First Price Auction</strong><br>
AAAI 2020 Workshop on Reinforcement Learning in Games. [<a href="http://aaai-rlg.mlanctot.info/papers/AAAI20-RLG_paper_9.pdf">paper</a>]</li>
</ol>
<h5 id="journal">Journal</h5>
<ol>
<li>阿部拳之, 豊島健太郎, 坂本充生, 岩崎敦.<br>
二人零和ゲームにおける突然変異駆動型正則化先導者追従法の終極反復収束<br>
情報処理学会論文誌. [<a href="https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&amp;page_id=13&amp;block_id=8&amp;item_id=234275&amp;item_no=1">paper</a>]</li>
</ol>
<h5 id="preprints">Preprints</h5>
<ol>
<li>Tsunehiko Tanaka, Kenshi Abe, Kaito Ariu, Tetsuro Morimura, Edgar Simo-Serra.<br>
<strong>Return-Aligned Decision Transformer</strong><br>
[<a href="https://arxiv.org/abs/2402.03923">Arxiv</a>]</li>
<li>Sho Shimoyama, Tetsuro Morimura, Kenshi Abe, Toda Takamichi, Yuta Tomomatsu, Masakazu Sugiyama, Asahi Hentona, Yuuki Azuma, Hirotaka Ninomiya.<br>
<strong>Why Guided Dialog Policy Learning performs well? Understanding the role of adversarial learning and its alternative</strong><br>
[<a href="https://arxiv.org/abs/2307.06721">Arxiv</a>]</li>
<li>Masahiro Kato, Kenshi Abe, Kaito Ariu, Shota Yasui.<br>
<strong>A Practical Guide of Off-Policy Evaluation for Bandit Problems</strong><br>
[<a href="https://arxiv.org/abs/2010.12470">Arxiv</a>]</li>
<li>Masahiro Nomura, Kenshi Abe.<br>
<strong>A Simple Heuristic for Bayesian Optimization with A Low Budget</strong><br>
[<a href="https://arxiv.org/abs/1911.07790">Arxiv</a>]</li>
</ol>
<h5 id="domestic-conference">Domestic Conference</h5>
<ol>
<li>坂本充生, 阿部拳之, 蟻生開人, 岩崎敦.<br>
RLHFにおける分布シフトの評価.<br>
<a href="https://www.ai-gakkai.or.jp/jsai2024/">第38回人工知能学会全国大会 (JSAI2024)</a>. [<a href="https://www.jstage.jst.go.jp/article/pjsai/JSAI2024/0/JSAI2024_1B3GS202/_article/-char/ja/">paper</a>]</li>
<li>石橋宙希, 島野雄貴, 阿部拳之, 岩崎敦.<br>
二人零和マルコフゲームにおける状態抽象化法に関する研究.<br>
<a href="https://www.ipsj.or.jp/event/taikai/86/">情報処理学会 第86回全国大会</a> [<a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&amp;active_action=repository_view_main_item_detail&amp;item_id=235852&amp;item_no=1&amp;page_id=13&amp;block_id=8">paper</a>].</li>
<li>板垣圭知, 小宮山純平, 阿部拳之, 岩崎敦.<br>
研修医配属における地域間格差を調整する制約のモンテカルロ木探索.<br>
<a href="https://www.ipsj.or.jp/event/taikai/86/">情報処理学会 第86回全国大会</a> [<a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&amp;active_action=repository_view_main_item_detail&amp;item_id=235850&amp;item_no=1&amp;page_id=13&amp;block_id=8">paper</a>].</li>
<li>阿部拳之, 蟻生開人, 坂本充生, 岩崎敦.<br>
A Slingshot Approach to Learning in Monotone Games.<br>
<a href="https://ibisml.org/ibis2023/">第25回情報論的学習理論ワークショップ (IBIS2023)</a>.</li>
<li>坂本充生, 阿部拳之, 蟻生開人, 岩崎敦.<br>
Zero-Variance Perturbation Utiity for Extensive-Form Games.<br>
<a href="https://ibisml.org/ibis2023/">第25回情報論的学習理論ワークショップ (IBIS2023)</a>.</li>
<li>藤本悠雅, 蟻生開人, 阿部拳之.<br>
Learning in Multi-Memory Games Triggers Complex Dynamics Diverging from Nash Equilibrium.<br>
<a href="https://ibisml.org/ibis2023/">第25回情報論的学習理論ワークショップ (IBIS2023)</a>.</li>
<li>山田博瑛, 小宮山純平, 阿部拳之, 岩崎敦.<br>
オンライン環境において公平な資源配分を実現するアルゴリズムに関する研究.<br>
<a href="https://ibisml.org/ibis2023/">第25回情報論的学習理論ワークショップ (IBIS2023)</a>.</li>
<li>山田博瑛, 小宮山純平, 阿部拳之, 岩崎敦.<br>
オンライン環境において公平な資源配分を実現するアルゴリズムに関する研究.<br>
<a href="https://www.ipsj.or.jp/event/fit/fit2023/abstract/data/html/program/f.html">第22回情報科学技術フォーラム (FIT 2023)</a>.</li>
<li>板垣圭知, 小宮山純平, 阿部拳之, 岩崎敦.<br>
研修医配属における地域間格差を調整するための制約のモンテカルロ木探索.<br>
<a href="https://www.ipsj.or.jp/event/fit/fit2023/abstract/data/html/program/f.html">第22回情報科学技術フォーラム (FIT 2023)</a>.</li>
<li>戸田隆道, 森村哲郎, 阿部拳之.<br>
タスク指向対話システムの方策学習への Decision Transformerの適用.<br>
<a href="https://www.anlp.jp/nlp2023/">言語処理学会 第29回年次大会 (NLP2023)</a> [<a href="https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_dir/Q12-6.pdf">paper</a>].</li>
<li>下山翔, 森村哲郎, 阿部拳之.<br>
タスク指向対話における強化学習を用いた対話方策学習への敵対的学習の役割の解明.<br>
<a href="https://www.anlp.jp/nlp2023/">言語処理学会 第29回年次大会 (NLP2023)</a> [<a href="https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_dir/Q12-5.pdf">paper</a>].</li>
<li>山田博瑛, 小宮山純平, 阿部拳之, 岩崎敦.<br>
オンライン環境において公平な資源配分を実現するアルゴリズムに関する研究.<br>
<a href="https://www.ipsj.or.jp/event/taikai/85/">情報処理学会 第85回全国大会</a> [<a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&amp;active_action=repository_view_main_item_detail&amp;item_id=230043&amp;item_no=1&amp;page_id=13&amp;block_id=8">paper</a>].</li>
<li>板垣圭知, 小宮山純平, 阿部拳之, 岩崎敦.<br>
研修医配属における地域間格差を調整するための制約のモンテカルロ木探索.<br>
<a href="https://www.ipsj.or.jp/event/taikai/85/">情報処理学会 第85回全国大会</a> [<a href="https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&amp;page_id=13&amp;block_id=8&amp;item_id=230050&amp;item_no=1">paper</a>].</li>
<li>坂本充生, 阿部拳之, 蟻生開人, 岩崎敦.<br>
二人零和展開型ゲームにおける突然変異付き乗算型重み更新に関する研究.<br>
<a href="https://www.ai-gakkai.or.jp/jsai2023/">第37回人工知能学会全国大会 (JSAI2023)</a>. [<a href="https://www.jstage.jst.go.jp/article/pjsai/JSAI2023/0/JSAI2023_2T4GS502/_article/-char/ja/">paper</a>]</li>
<li>山田博瑛, 小宮山純平, 阿部拳之, 岩崎敦.<br>
オンライン環境において公平な資源配分を実現するアルゴリズムに関する研究.<br>
<a href="https://www.ai-gakkai.or.jp/jsai2023/">第37回人工知能学会全国大会 (JSAI2023)</a>. [<a href="https://www.jstage.jst.go.jp/article/pjsai/JSAI2023/0/JSAI2023_2T4GS503/_article/-char/ja/">paper</a>]</li>
<li>阿部拳之, 蟻生開人, 坂本充生, 豊島健太郎, 岩崎敦.<br>
Last-Iterate Convergence with Full- and Noisy-Information Feedback in Two-Player Zero-Sum Games.<br>
<a href="https://ibisml.org/ibis2022/">第24回情報論的学習理論ワークショップ (IBIS2022)</a>.</li>
<li>蟻生開人, 阿部拳之, Alexandre Proutiere.<br>
Thresholded Lasso Bandit.<br>
<a href="https://ibisml.org/ibis2022/">第24回情報論的学習理論ワークショップ (IBIS2022)</a>.</li>
<li>富樫陸, 阿部拳之.<br>
公平性を考慮した大規模推薦システム.<br>
<a href="https://ibisml.org/ibis2022/">第24回情報論的学習理論ワークショップ (IBIS2022)</a>.</li>
<li>森村哲郎, 大田和寛, 阿部拳之, 張培楠.<br>
ビームサーチ推論のための強化学習.<br>
<a href="https://ibisml.org/ibis2022/">第24回情報論的学習理論ワークショップ (IBIS2022)</a>.</li>
<li>豊島健太郎, 坂本充生, 阿部拳之, 岩崎敦.<br>
二人零和ゲームにおける突然変異駆動型Follow-The-Regularized-Leaderの終極反復収束.<br>
<a href="https://www.ipsj.or.jp/event/fit/fit2022/FIT2022_program/data/html/program/f.html">第21回情報科学技術フォーラム (FIT 2022)</a>.</li>
<li>坂本充生, 豊島健太郎, 阿部拳之, 岩崎敦.<br>
二人零和ゲームにおける突然変異付きレプリケータダイナミクスを用いた学習アルゴリズムに関する研究.<br>
<a href="https://www.ai-gakkai.or.jp/jsai2022/">第36回人工知能学会全国大会 (JSAI2022)</a>. [<a href="https://www.jstage.jst.go.jp/article/pjsai/JSAI2022/0/JSAI2022_2O6GS502/_pdf/-char/ja">paper</a>]</li>
<li>坂本充生, 阿部拳之, 岩崎敦.<br>
二人零和ゲームにおける突然変異付きレプリケータダイナミクスを用いた学習アルゴリズムに関する研究.<br>
<a href="https://www.ipsj.or.jp/event/taikai/84/index.html">情報処理学会 第84回全国大会</a>.</li>
<li>豊島健太郎, 坂本充生, 阿部拳之, 岩崎敦.<br>
クールノー競争におけるマルチエージェント強化学習に関する研究.<br>
<a href="https://www.ipsj.or.jp/event/taikai/84/index.html">情報処理学会 第84回全国大会</a>.</li>
<li>坂本充生, 阿部拳之, 岩崎敦.<br>
見間違えのある繰り返しゲームのためのActor-Critic型強化学習.<br>
<a href="https://ibisml.org/ibis2021/">日本オペレーションズ・リサーチ学会 2021年 秋季研究発表会</a>. [<a href="https://orsj.org/nc2021f/wp-content/uploads/sites/2/2021/08/2021f-2-C-11.pdf">paper</a>]</li>
<li>坂本充生, 阿部拳之, 岩崎敦.<br>
見間違えのある繰り返しゲームのためのActor-Critic型強化学習.<br>
<a href="https://ibisml.org/ibis2021/">第24回情報論的学習理論ワークショップ (IBIS2021)</a>.</li>
<li>島野雄貴, 阿部拳之, 岩崎敦, 大河原一憲.<br>
反実仮想後悔最小化によるアメリカンフットボールにおけるオフェンス戦略の均衡推定.<br>
<a href="https://www.ipsj.or.jp/event/fit/fit2021/FIT2021_program/data/html/program/f.html">第20回情報科学技術フォーラム (FIT 2021)</a>.</li>
<li>坂本充生, 阿部拳之, 岩崎敦.<br>
見間違えのある繰り返し囚人のジレンマにおける方策勾配法に関する研究.<br>
<a href="https://www.ipsj.or.jp/event/fit/fit2021/FIT2021_program/data/html/program/f.html">第20回情報科学技術フォーラム (FIT 2021)</a> (FIT船井ベストペーパー賞). [<a href="https://www.ipsj.or.jp/award/9faeag0000004eyo-att/CF-002.pdf">paper</a>]</li>
<li>阿部拳之, 金子雄祐.<br>
二人零和マルコフゲームにおけるオフ方策評価のためのQ学習.<br>
<a href="https://www.logos.ic.i.u-tokyo.ac.jp/~tsuruoka/sig-gi/gpw/2020/index.html">第25回ゲームプログラミングワークショップ (GPW 2020)</a>. <a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&amp;active_action=repository_view_main_item_detail&amp;item_id=207671&amp;item_no=1&amp;page_id=13&amp;block_id=8">[paper]</a></li>
<li>阿部拳之.<br>
花札におけるナッシュ均衡戦略の計算.<br>
<a href="http://ibisml.org/ibis2019/">第22回情報論的学習理論ワークショップ (IBIS2019)</a>.</li>
<li>野村将寛, 阿部拳之.<br>
Black-box最適化に対するBudgetを考慮した探索空間の初期化.<br>
<a href="https://www.ai-gakkai.or.jp/jsai2019/">第33回人工知能学会全国大会 (JSAI 2019)</a>. [<a href="https://www.jstage.jst.go.jp/article/pjsai/JSAI2019/0/JSAI2019_4Rin102/_article/-char/ja">paper</a>]</li>
<li>阿部拳之, 野村将寛.<br>
非定常多腕バンディットアルゴリズムを用いたハイパーパラメータ最適化フレームワークの提案.<br>
<a href="http://ibisml.org/ibis2018/">第21回情報論的学習理論ワークショップ (IBIS2018)</a>.</li>
<li>阿部拳之, 小野功.<br>
活用と探索の釣り合いを考慮した事例ベース政策最適化.<br>
<a href="http://www.jpnsec.org/symposium201701.html">第12回進化計算学会研究会 (2017年)</a>. (ベストポスター発表賞)</li>
<li>阿部拳之, 小野功.<br>
多峰性景観下での自然進化戦略による事例ベース政策最適化.<br>
<a href="https://www.sice.or.jp/org/SSI2016/">計測自動制御学会システム・情報部門学術講演会 (SSI2016)</a>.</li>
<li>阿部拳之, 小野功.<br>
自然進化戦略を用いた事例ベース政策最適化.<br>
<a href="https://www.sice.or.jp/system/system_ken54.html">第54回システム工学部会研究会 (2016年)</a>.</li>
</ol>
<h1 id="presentations">Presentations</h1>
<ol>
<li><strong>Adaptively Perturbed Mirror Descent for Learning in Games</strong><br>
<a href="https://cyberagent.connpass.com/event/320641/">ICML 2024 著者発表会</a> (2024/7/10).</li>
<li>機械学習が紡ぐゲーム理論のフロンティア<br>
<a href="https://www.ai-gakkai.or.jp/jsai2024/ks#ks-1">第38回人工知能学会全国大会 (JSAI 2024) 企画セッション</a> (2024/5/29).</li>
<li><a href="https://speakerdeck.com/bakanaouji/learning-in-games">Learning in games: ゲーム理論とオンライン学習</a><br>
<a href="https://cyberagent.connpass.com/event/286764/">Algorithmic Learning &amp; Optimization 勉強会#1</a> (2023/6/24).</li>
<li><a href="https://www.slideshare.net/KenshiAbe/ss-248654457">二人零和マルコフゲームにおけるオフ方策評価</a><br>
<a href="https://connpass.com/event/212654/">AAMAS2021 著者発表会</a> (2021/5/27).</li>
<li>広告配信オークションにおける入札戦略<br>
<a href="https://www.ipsj.or.jp/event/fit/fit2020/splist-AITECHTALK.html">第19回情報科学技術フォーラム (FIT 2020)</a> (2020/9/1).</li>
<li>多人数不完全情報ゲームにおけるAI開発<br>
<a href="https://www.jeameetings.org/2020s/index.html">日本経済学会 2020年度春季大会</a> (2020/5/30).</li>
<li><a href="https://www.slideshare.net/KenshiAbe/ai-165308197">多人数不完全情報ゲームにおけるAI ~ポーカーと麻雀を例として~</a><br>
<a href="https://rlarch.connpass.com/event/143128/">第43回強化学習アーキテクチャ勉強会</a> (2019/8/20).</li>
</ol>
<h1 id="blog-posts">Blog Posts</h1>
<ul>
<li><a href="https://qiita.com/bakanaouji/items/f70d7948931c96d94ef8">【ゲーム理論】展開型ゲームのナッシュ均衡を計算しよう：Counterfactual Regret Minimizationの解説</a></li>
<li><a href="https://qiita.com/bakanaouji/items/fefa93cc53cafbdd985d">【Unity ML-Agents】 Self-Play Reinforcement Learningで対戦ゲームのAIを作ってみた</a></li>
<li><a href="https://qiita.com/bakanaouji/items/d20c8903a1327e660de5">Q-Learningがどの程度Off-Policyなのかを調べてみた</a></li>
<li><a href="https://cyberagent.ai/blog/research/2522/">ミニ花札のAIを作ってみよう</a></li>
<li><a href="https://qiita.com/bakanaouji/items/aa076cef1e04f77f48ce">遺伝的アルゴリズムでコードフォーマッタのスタイルを最適化する</a></li>
<li><a href="https://qiita.com/bakanaouji/items/75444b4d97ede83c7c48">max k-armed banditとは？</a></li>
<li><a href="https://cyberagent.ai/blog/research/1036">Successive Halvingの性能解析</a></li>
</ul>

  </article>
</section>


      </div>
      
        <footer class="footer">
  <section class="container">
    
      <div class="sns-shares sp-sns-shares">
        
        
        
        
        
      </div>
    
    
     © 0001    ·  Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/naro143/hugo-coder-portfolio">CoderPortfolio</a>. 

  </section>
</footer>
<div class="fixed-bar">
  <section class="container">
    
      <p id="privateTriggerText">Do you want to get in touch with me? →<a id="privateTrigger">Click!</a></p>
    
    
      <div class="sns-shares pc-sns-shares">
        
        
        
        
        
      </div>
    
  </section>
</div>

      
    </main>

    
<script>
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-150920247-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

  <script src="/js/app.js"></script>
  
  <script>
  (function($) {
    $(function() {
      $('#privateTrigger').on('click', function() {
        $('.private').slideToggle();
        $('#privateTriggerText').text("Thank You! Please share it if you like it→");
      });
    });
   })(jQuery);
  </script>
  
  </body>
</html>
